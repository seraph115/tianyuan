# docker-compose.yml — Redis + Kafka 4.1.0 (KRaft) + Scrapy crawler
services:
  redis:
    image: redis:7-alpine
    command: ["redis-server", "--appendonly", "yes"]
    ports: ["6379:6379"]
    volumes:
      - redis_data:/data
    networks:
      - crawler_net

  kafka-setup:
    image: apache/kafka:4.1.0
    volumes:
      - kafka_data:/var/lib/kafka
    entrypoint: ["/bin/bash","-lc"]
    command: |
      set -euo pipefail
      test -f /etc/kafka/docker/server.properties
      if [ ! -f "/var/lib/kafka/data/meta.properties" ]; then
        CLUSTER_ID=$(/opt/kafka/bin/kafka-storage.sh random-uuid)
        echo "$CLUSTER_ID" > /var/lib/kafka/cluster.id
        /opt/kafka/bin/kafka-storage.sh format \
          -t "$CLUSTER_ID" \
          -c /etc/kafka/docker/server.properties
        echo "Formatted KRaft storage with CLUSTER_ID=$CLUSTER_ID"
      else
        echo "KRaft storage already formatted"
      fi
    restart: "no"
    networks:
      - crawler_net

  kafka:
    image: apache/kafka:4.1.0
    depends_on: [kafka-setup]
    ports:
      - "9092:9092"    # 容器内互联/本机直连
      - "29092:29092"  # 仅本机/外部直连（可选）
      - "9093:9093"    # controller
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_LISTENERS: "PLAINTEXT://:9092,PLAINTEXT_HOST://:29092,CONTROLLER://:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_DIRS: "/var/lib/kafka/data"
    volumes:
      - kafka_data:/var/lib/kafka
    networks:
      - crawler_net

  crawler:
    build:
      context: ./spider
      dockerfile: Dockerfile
    environment:
      - REDIS_URL=redis://redis:6379/0
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC_ITEMS=items.out
      - PLAYWRIGHT_BROWSERS_PATH=/ms-playwright
    depends_on: [redis, kafka]
    command: ["bash","-lc","scrapy crawl example -s LOG_LEVEL=INFO"]
    networks:
      - crawler_net

volumes:
  redis_data:
  kafka_data:

networks:
  crawler_net:
    external: true
